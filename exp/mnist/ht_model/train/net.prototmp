{% set actual_batch_size = ((batch_size / number_of_gpus) | int) %}
{% macro sim_layer(name, bottom, top, num_instances, block_size, padding=0, stride=1, input_channels=3) %}
     layer {
        name: "{{ name }}"
        type: "Similarity"
        bottom: "{{ bottom }}"
        top: "{{ top }}"
        param {
            lr_mult: {{ sim_templates_lr | float }}
            decay_mult: 0
            min_value: -0.5
            max_value: 0.5
        }
        param {
            lr_mult: {{ sim_weights_lr | float }}
            min_value: 0
            decay_mult: {{ sim_weight_decay | float }}
        }
        similarity_param {
            similarity_function: L2
            num_instances: {{ num_instances | int }} 
            bias_term: false
            normalization_term: true
            normalization_term_fudge: 1e-4
            ignore_nan_input: true
            block_param {
                block_size: {{ block_size | int }}
                pad: {{ padding | int }}
                stride: {{ stride | int }}
                out_of_bounds_value: nan
            }
{#             unsupervised_init {
                type: "gmm"
                max_iterations: 10
                num_batches: 400
                fudge_factor: 1e-4
            } #}
            template_filler {
                type: "gaussian"
                mean: 0
                std: 0.05
            }
            weight_filler {
                type: "gaussian"
                mean: 100
                std: 0
            }
        }
    }
{% endmacro %}
{% macro mex_layer(name, bottom, top, num_instances, input_channels, use_unshared, region_size, filler_alpha=1.0, lr=1.0) %}
    layer {
        name: "{{ name }}"
        type: "MEX"
        bottom: "{{ bottom }}"
        top: "{{ top }}"
        param {
            lr_mult: 0
            decay_mult: 0
        }
        param {
            lr_mult: {{ (mex_offsets_lr*lr) | float }}
            decay_mult: 1
            is_logspace: {{ log_space | bool }}
        }
        mex_param {
            num_instances: {{ num_instances | int }}
            softmax_mode: true
            normalize_offsets: true
            normalize_offsets_projected: {{ projected_offsets | bool }}
            use_unshared_regions: {{ use_unshared | bool }}
            {% if use_unshared -%}
            unshared_offsets_region_size: {{ region_size | int }}
            {% else -%}
            shared_offsets_region_size: {{ region_size | int }}
            {%- endif %}
            use_log_space_parameters: {{ log_space | bool }}
	    linear_space_min_value: {{ linear_min_value }}
            block_param {
                block_size: 1
                stride: 1
            }
            epsilon_filler {
                type: "constant"
                value: 1
            }
            offsets_filler {
                type: "dirichlet"
                alpha: {{ filler_alpha }}
                primal_dim: {{ input_channels | int }}
                to_log: {{ log_space | bool }}
            }
        }
    }
{% endmacro %}

{% macro mexpool_layer(name, bottom, top, num_instances, input_channels, use_unshared, shared_region_size, pool_size, pad=0,lr=1.0) -%}
{{ mex_layer(name ~ '_mex', bottom, name ~ '_mex', num_instances, input_channels, use_unshared, shared_region_size, lr=lr) }}
{{  pooling_layer(name ~ '_pool', name ~ '_mex', top, size=pool_size, stride=pool_size, pad=pad) }}
{%- endmacro %}

{% macro mexpool_normed_layer(name, bottom, top, num_instances, input_channels, use_unshared, shared_region_size, pool_size, pad=0, bottom_norm=None, top_norm=None, lr=1.0) -%}
{{ norm_layer(name ~ '_norm', bottom, name ~ '_normalized', name ~ '_norm', input_channels) }}
{{ mexpool_layer(name, name ~ '_normalized', (name ~ '_shifted') if top_norm == None else top, num_instances, input_channels, use_unshared, shared_region_size, pool_size, pad, lr=lr) }}
    layer {
        name: "{{ name ~ '_post_norm' }}"
        type: "Pooling"
        bottom: "{{ name ~ '_norm' }}"
{% if bottom_norm != None %}
        top: "{{ name ~ '_original_post_norm' }}"
{% else %}
        top: "{{ name ~ '_post_norm' if top_norm == None else top_norm }}"
{% endif %}
        pooling_param {
            pool: SUM
            engine: CAFFE
            kernel_size: {{ pool_size | int }}
            stride: {{ pool_size | int }}
            pad: {{ pad | int }}
        }
    }
{% if bottom_norm != None %}
    layer {
        name: "{{ name ~ '_post_norm' }}"
        type: "Pooling"
        bottom: "{{ bottom_norm }}"
        top: "{{ name ~ '_bottom_post_norm' }}"
        pooling_param {
            pool: SUM
            engine: CAFFE
            kernel_size: {{ pool_size | int }}
            stride: {{ pool_size | int }}
            pad: {{ pad | int }}
        }
    }
{% endif %}
{% if bottom_norm != None %}
    layer {
        name: "{{ name ~ '_combined_post_norm' }}"
        type: "Eltwise"
        bottom: "{{ name ~ '_original_post_norm' }}"
        bottom: "{{ name ~ '_bottom_post_norm' }}"
        top: "{{ name ~ '_post_norm' if top_norm == None else top_norm }}"
        eltwise_param{
            operation: SUM
            coeff: 1
            coeff: 1
        }
    }
{% endif %}
{% if top_norm == None %}
    layer {
        name: "{{ name ~ '_post_norm_tile' }}"
        type: "Tile"
        bottom: "{{ name ~ '_post_norm' }}"
        top: "{{ name ~ '_post_norm_tile' }}"
        tile_param{
            axis: 1
            tiles: {{ num_instances }}
        }
    }
    layer {
        name: "{{ name ~ '_denormalized' }}"
        type: "Eltwise"
        bottom: "{{ name ~ '_shifted' }}"
        bottom: "{{ name ~ '_post_norm_tile' }}"
        top: "{{ top }}"
        eltwise_param{
            operation: SUM
            coeff: 1
            coeff: 1
        }
    }
{% endif %}
{% endmacro %}
{% macro norm_layer(name, bottom, top, top_norm, input_channels) %}
    layer {
        name: "{{ name }}"
        type: "MEX"
        bottom: "{{ bottom }}"
        top: "{{ top_norm }}"
        param {
            lr_mult: 0
            decay_mult: 0
        }
        param {
            lr_mult: 0
            decay_mult: 0
        }
        mex_param {
            num_instances: 1
            softmax_mode: true
            normalize_offsets: false
            shared_offsets_region_size: -1
            block_param {
                block_size: 1
                stride: 1
            }
            epsilon_filler {
                type: "constant"
                value: 1
            }
            offsets_filler {
                type: "constant"
                value: 0
            }
        }
    }
    layer {
        name: "{{ name ~ '_norm_tile' }}"
        type: "Tile"
        bottom: "{{ top_norm }}"
        top: "{{ name ~ '_norm_tile' }}"
        tile_param{
            axis: 1
            tiles: {{ input_channels }}
        }
    }
    layer {
        name: "{{ name ~ '_normalized' }}"
        type: "Eltwise"
        bottom: "{{ bottom }}"
        bottom: "{{ name ~ '_norm_tile' }}"
        top: "{{ top }}"
        eltwise_param{
            operation: SUM
            coeff: 1
            coeff: -1
        }
    }
{% endmacro %}
{% macro pooling_layer(name, bottom, top, size=2, stride=2, pad=0, type='SUM', global_pooling=False) %}
    layer {
        name: "{{ name }}"
        type: "Pooling"
        bottom: "{{ bottom }}"
        top: "{{ top }}"
        pooling_param {
            pool: {{ type }}
            {% if global_pooling -%}
            global_pooling: true
            {% else -%}
            kernel_size: {{ size | int }}
            stride: {{ stride | int }}
            pad: {{ pad | int }}
            {%- endif %}
            engine: CAFFE
        }
    }
{%- endmacro %}
{% macro random_marginalize_layer(name, bottom, top, prob_of_keeping, window_size, input_channels, bottom_norm=None, top_norm=None, to_nan=False) %}
    layer {
        name: "{{ name ~ '_margin_mask' }}"
        type: "DummyData"
        top: "{{ name ~ '_margin_mask' }}"
        dummy_data_param {
            data_filler {
                type: "bernoulli"
                non_zero_probability: {{ prob_of_keeping }}
            }
            shape {
                dim: {{ actual_batch_size | int }}
                dim: 1
                dim: {{ window_size | int }}
                dim: {{ window_size | int }}
            }
        }
        include {
            phase: TRAIN
        }
    }
    layer {
        name: "{{ name ~ '_margin_mask' }}"
        type: "DummyData"
        top: "{{ name ~ '_margin_mask' }}"
        dummy_data_param {
            data_filler {
                type: "constant"
                value: 1
            }
            shape {
                dim: 100
                dim: 1
                dim: {{ window_size | int }}
                dim: {{ window_size | int }}
            }
        }
        include {
            phase: TEST
        }
    }

    layer {
        name: "{{ name ~ '_to_nan' }}"
        type: "Power"
        bottom: "{{ name ~ '_margin_mask' }}"
        top: "{{ name ~ '_margin_mask_corrected' }}"
        power_param{
        {% if to_nan %}
            shift: -1
            power: 0.5
        {% endif %}
        }
    }
    layer {
        name: "{{ name ~ '_mask_tile' }}"
        type: "Tile"
        bottom: "{{ name ~ '_margin_mask_corrected' }}"
        top: "{{ name ~ '_margin_tile' }}"
        tile_param{
            axis: 1
            tiles: {{ input_channels }}
        }
    }
    layer {
        name: "{{ name ~ '_marginalize' }}"
        type: "Eltwise"
        bottom: "{{ bottom }}"
        bottom: "{{ name ~ '_margin_tile' }}"
        top: "{{ top }}"
        eltwise_param{
{% if to_nan %}
            operation: SUM
            coeff: 1
            coeff: 1
{% else %}
            operation: PROD
{% endif %}
        }
    }
{% if bottom_norm != None and top_norm != None %}
    layer {
        name: "{{ name ~ '_marginalize_norm' }}"
        type: "Eltwise"
        bottom: "{{ bottom_norm }}"
        bottom: "{{ name ~ '_margin_mask' }}"
        top: "{{ top_norm }}"
        eltwise_param{
            operation: PROD
        }
    }
{% endif %}
{%- endmacro %}
{% macro concat_layer(name, bottoms) %}
    layer {
        name: "{{ name }}"
        type: "Concat"
        {% for bottom in bottoms -%}
        bottom: "{{ bottom }}"
        {% endfor %}
        top: "{{ name }}"
    }
{%- endmacro %}
{% macro ll_loss_layer(name, bottom, weight, use_labeled_data=True) %}
    layer {
        name: "{{ name }}"
        type: "LogLikelihoodLoss"
        log_likelihood_loss_param {
           use_labeled_data: {{ use_labeled_data | bool }}
        }
        loss_weight: {{ weight }}
        bottom: "{{ bottom }}"
        bottom: "label"
        top: "{{ name }}"
    }
{%- endmacro %}
{% macro ll_side_loss(name, bottom, weight, use_labeled_data=True,shared_offsets=-1) %}
{% if weight > 0 -%}
    layer {
        name: "{{ name ~ '_mex' }}"
        type: "MEX"
        bottom: "{{ bottom }}"
        top: "{{ name ~ '_mex' }}"
        param {
            lr_mult: 0
            decay_mult: 0
        }
        param {
            lr_mult: 1
            decay_mult: 0
        }
        mex_param {
            num_instances: 1
            softmax_mode: true
            normalize_offsets: true
            shared_offsets_region_size: {{ shared_offsets | int }}
            block_param {
                block_size: 1
                stride: 1
            }
            epsilon_filler {
                type: "constant"
                value: 1
            }
            offsets_filler {
                type: "constant"
                value: 0
            }
        }
    }
    {{ pooling_layer(name ~ '_pooling', name ~ '_mex', name ~ '_pooling', global_pooling=True) }}
    layer {
        name: "{{ name ~ '_loss' }}"
        type: "SumLoss"
        sum_loss_param {
           use_labeled_data: {{ use_labeled_data | bool }}
        }
        loss_weight: {{ weight }}
        bottom: "{{ name ~ '_pooling' }}"
        bottom: "label"
        top: "{{ name ~ '_loss' }}"
    }
{%- endif %}
{%- endmacro %}
{% macro input(name, path, batch_size, add_mask, is_train, stage=None, shuffle=False) %}
   layer {
        name: "{{ name }}_labeled"
        type: "ImageData"
        top: "data"
        top: "label"
        image_data_param {
            source: "data/{{ path }}/index.txt"
            root_folder: "data/{{ path }}/"
            batch_size: {{ batch_size | int }}
            shuffle: {{ shuffle | bool }}
            is_color: false
            cache_images: true
        }
        transform_param {
            scale: {{ 1.0 / 255.0 }}
            mean_value: {{ 255.0 / 2.0 }}
        }
        include: { 
            phase: {{ 'TRAIN' if is_train else 'TEST' }}
            {{ ('stage: "%s"' | format(stage)) if stage }}
        }
    }
{% endmacro %}
{# END OF MACRO DECLARATION #}
# Number of iterations between tests
test_interval: 1000
# Covering the full 10,000 testing and 40,000 training images.
test_iter: 100
test_state: { stage: "test-on-valid-set" }
test_iter: 100
test_state: { stage: "test-on-train-set" }
# Unsupervised test settings
init_test_interval: 0 # How many iterations between each test
init_test_iter: 625   # number of batches per test - 625*64=40000
# The base learning rate, momentum and the weight decay of the network.
base_lr: {{ base_lr }}
momentum: {{ momentum }}
momentum2: {{ momentum2 }}
delta: {{ delta }}
weight_decay: {{ weight_decay }}
# The learning rate policy
lr_policy: "multistep"
stepvalue: {{ (max_iter * 0.8) | int }}
step_lr: {{ base_lr }}
step_policy: "constant"
stepvalue: {{ max_iter | int }}
step_lr: 0
step_policy: "linear"
#gamma: 0.1
# Number of iterations between displays
display: 20
# The maximum number of iterations
max_iter: {{ max_iter | int }}
iter_size: 1
# snapshot intermediate results
snapshot_prefix: "{{ name }}"
snapshot: 0
snapshot_after_train: true
# solver mode: CPU or GPU
solver_mode: GPU
type: "{{ solver_type }}"
debug_info: false
# For reproducibility:
random_seed: {{ seed }} # 314159

net_param {
    name: "{{ name }}"
    ##############
    ### Source ###
    ##############
{{ input('mnist_train', 'mnist_train_image_data', actual_batch_size, is_train=True, stage=None) }}
{{ input('mnist_test_valid', 'mnist_validation_image_data', 100, is_train=False, stage='test-on-valid-set') }}
{{ input('mnist_test_train', 'mnist_train_image_data', 100, is_train=False, stage='test-on-train-set', shuffle=True) }}
{{ random_marginalize_layer('rm0', 'data', 'rm0', rm_prob0 if rm_active0 else 1, 28, 1, to_nan=True) }}
{{ sim_layer('sim1', 'rm0', 'sim1', sim1_num_instances, 2, stride=2, input_channels=1, padding=2) }}
{{ random_marginalize_layer('rm1', 'sim1', 'rm1', rm_prob1 if rm_active1 else 1, 16, sim1_num_instances, to_nan=False) }}
{{ mexpool_normed_layer('lv1', 'rm1', 'lv1', lv1_num_instances, sim1_num_instances, true, 2, 2, top_norm='lv1_top_norm') }}
{{ random_marginalize_layer('rm2', 'lv1', 'rm2', rm_prob2 if rm_active2 else 1, 8, lv1_num_instances, to_nan=False, bottom_norm='lv1_top_norm', top_norm='rm2_top_norm') }}
{{ mexpool_normed_layer('lv2', 'rm2', 'lv2', lv2_num_instances, lv1_num_instances, false, 1, 2, top_norm='lv2_top_norm', bottom_norm='rm2_top_norm') }}
{{ random_marginalize_layer('rm3', 'lv2', 'rm3', rm_prob3 if rm_active3 else 1, 4, lv2_num_instances, to_nan=False, bottom_norm='lv2_top_norm', top_norm='rm3_top_norm') }}
{{ mexpool_normed_layer('lv3', 'rm3', 'lv3', lv3_num_instances, lv2_num_instances, false, 1, 2, top_norm='lv3_top_norm', bottom_norm='rm3_top_norm') }}
{{ mexpool_normed_layer('lv4', 'lv3', 'lv4', lv4_num_instances, lv3_num_instances, false, 1, 2, top_norm='lv4_top_norm', bottom_norm='lv3_top_norm') }}
{{ mexpool_normed_layer('lv5', 'lv4', 'lv5', 10, lv4_num_instances, false, 1, 1, top_norm='lv5_top_norm', bottom_norm='lv4_top_norm') }}
{{ pooling_layer('global_sum_pool', 'lv5', 'unnormalized', global_pooling=True) }}
{{ pooling_layer('global_sum_pool2', 'lv5_top_norm', 'total_norm', global_pooling=True) }}

    ############
    ### Sink ###
    ############

    layer {
        name: "loss"
        type: "GeneralizedHingeLoss"
        generalized_hinge_loss_param {
            epsilon: 1
            margin: 0
        }
        loss_weight: 1
        bottom: "unnormalized"
        bottom: "label"
        top: "loss"
    }
    layer {
        name: "unnormalized_prob"
        type: "MEX"
        bottom: "unnormalized"
        top: "unnormalized_prob"
        param {
            lr_mult: 0
            decay_mult: 0
        }
        param {
            lr_mult: 0
            decay_mult: 0
        }
        mex_param {
            num_instances: 1
            softmax_mode: true
            normalize_offsets: true
            shared_offsets_region_size: -1
            block_param {
                block_size: 1
                stride: 1
            }
            epsilon_filler {
                type: "constant"
                value: 1
            }
            offsets_filler {
                type: "constant"
                value: 0
            }
        }
    }
    layer {
        name: "likelihood"
        type: "Eltwise"
        bottom: "unnormalized_prob"
        bottom: "total_norm"
        top: "likelihood"
        eltwise_param{
            operation: SUM
            coeff: 1
            coeff: 1
        }
    }
    layer {
        name: "likelihood_loss"
        type: "SumLoss"
        sum_loss_param {
           use_labeled_data: true
        }
        loss_weight: {{ likelihood_loss_weight }}
        bottom: "likelihood"
        bottom: "label"
        top: "likelihood_loss"
    }
    layer {
        name: "accuracy"
        type: "Accuracy"
        bottom: "unnormalized"
        bottom: "label"
        top: "accuracy"
        include: { phase: TEST }
    }
}
